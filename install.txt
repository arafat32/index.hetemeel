
# Whisper

[[Блог]](https://openai.com/blog/whisper)
[[Бумага]](https://cdn.openai.com/papers/whisper.pdf)
[[Карточка модели]](модель -card.md)
[[Пример Colab]](https://colab.research.google.com/github/openai/whisper/blob/master/notebooks/LibriSpeech.ipynb)

Whisper — это модель распознавания речи общего назначения. Он обучен на большом наборе данных разнообразного аудио, а также является многозадачной моделью, которая может выполнять многоязычное распознавание речи, а также перевод речи и идентификацию языка.


## Подход

![Подход](подход.png)

Модель преобразования последовательности к последовательности обучается различным задачам обработки речи, включая распознавание многоязычной речи, перевод речи, идентификацию разговорного языка и обнаружение голосовой активности. Все эти задачи совместно представлены в виде последовательности токенов, которые должны быть предсказаны декодером, что позволяет одной модели заменить множество различных этапов традиционного конвейера обработки речи. Формат многозадачного обучения использует набор специальных токенов, которые служат спецификаторами задач или целями классификации.


## Настраивать

Мы использовали Python 3.9.9 и [PyTorch](https://pytorch.org/) 1.10.1 для обучения и тестирования наших моделей, но ожидается, что кодовая база будет совместима с Python 3.7 или более поздней версии и последними версиями PyTorch. Кодовая база также зависит от нескольких пакетов Python, в первую очередь [HuggingFace Transformers] (https://huggingface.co/docs/transformers/index) для их быстрой реализации токенизатора и [ffmpeg-python] (https://github.com). /kkroening/ffmpeg-python) для чтения аудиофайлов. Следующая команда извлечет и установит последнюю фиксацию из этого репозитория вместе с зависимостями Python

     pip install git+https://github.com/openai/whisper.git

. https://ffmpeg.org/) для установки в вашей системе, которая доступна в большинстве менеджеров пакетов:

```bash
# в Ubuntu или Debian
sudo apt update && sudo apt install ffmpeg

# в Arch Linux
sudo pacman -S ffmpeg

# в MacOS с использованием Homebrew (https://brew.sh/)
brew install ffmpeg

# в Windows с использованием Chocolatey (https:// Chocolatey.org/)
choco install ffmpeg

# в Windows с помощью Scoop (https://scoop.sh/)
scoop install ffmpeg
```

Вам также может понадобиться установить [`rust`](http://rust-lang.org), если [tokenizers](https://pypi.org/project/tokenizers/) не предоставляет встроенное колесо для вашей платформы. Если вы видите ошибки установки во время команды `pip install` выше, перейдите на [Страница начала работы] (https://www.rust-lang.org/learn/get-started), чтобы установить среду разработки Rust. Кроме того, вам может потребоваться настроить переменную среды `PATH`, например, `export PATH="$HOME/.cargo/bin:$PATH"`. Если установка завершилась с ошибкой «Нет модуля с именем 'setuptools_rust'», вам необходимо установить `setuptools_rust`, например, запустив:

```bash
pip install setuptools-rust
```


## Доступные модели и языки

Существует пять размеров моделей, четыре версии только на английском языке, предлагающие компромисс между скоростью и точностью. Ниже приведены названия доступных моделей, их примерные требования к памяти и относительная скорость.


| Размер | Параметры | англоязычная модель | Многоязычная модель | Требуемая видеопамять | Относительная скорость |
|:------:|:----------:|:----:|:----- -------------:|:--------------:|:--------------:|
| крошечный | 39 М | `маленький.ru` | `маленький` | ~1 ГБ | ~32x |
| база | 74 М | `base.ru` | `база` | ~1 ГБ | ~16x |
| маленький | 244 М | `small.ru` | `маленький` | ~2 ГБ | ~6x |
| средний | 769 М | `medium.ru` | `средний` | ~5 ГБ | ~2x |
| большой | 1550 м | Н/Д | `большой` | ~10 ГБ | 1x |

Для приложений только на английском языке модели .en работают лучше, особенно для моделей `tiny.en` и `base.en`. Мы заметили, что разница становится менее значимой для моделей small.en и medium.en.

Производительность Whisper сильно различается в зависимости от языка. На рисунке ниже показана разбивка WER по языкам набора данных Fleurs с использованием «большой» модели. Дополнительные оценки WER и BLEU, соответствующие другим моделям и наборам данных, можно найти в Приложении D к [документу] (https://cdn.openai.com/papers/whisper.pdf).

![Разбивка WER по языкам](language-breakdown.svg)



## Использование командной строки

Следующая команда будет транскрибировать речь в аудиофайлах, используя модель ` medium`

     :

хорошо для транскрипции английского языка. Чтобы транскрибировать аудиофайл, содержащий неанглийскую речь, вы можете указать язык с помощью параметра --language:

     шепот japanese.wav --language Японский

Добавление `--task translate` переведет речь на английский:

     шепот японский. wav --language Japanese --task translate

Выполните следующее, чтобы просмотреть все доступные параметры:

     шепот --help

См. [tokenizer.py](whisper/tokenizer.py) для получения списка всех доступных языков.


## Использование Python



Транскрипцию также можно выполнить в
Python :

_ метод `transcribe()` считывает весь файл и обрабатывает аудио со скользящим 30-секундным окном, выполняя авторегрессионные прогнозы от последовательности к последовательности в каждом окне. Ниже приведен пример использования `whisper.detect_language()` и `whisper.decode()`, которые обеспечивают доступ к модели более низкого уровня. ```python importshed шепот модель = шептать.load_model("база") # загрузить аудио и дополнить / обрезать его, чтобы он соответствовал 30 секундам audio = шептать.load_audio("audio.mp3")
